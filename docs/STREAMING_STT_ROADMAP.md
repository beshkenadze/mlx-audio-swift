# ğŸ¯ Native Swift STT with AlignAtt Streaming

> **Design Document**: [2025-01-06-streaming-stt-design.md](plans/2025-01-06-streaming-stt-design.md)
>
> **Status**: Design approved, ready for implementation

## Overview

This document outlines the roadmap for implementing native Speech-to-Text (STT) with AlignAtt streaming support in MLX Swift, enabling low-latency transcription on Apple Silicon.

## Current Landscape

| Aspect | WhisperKit | Our Implementation (Python) |
|--------|------------|--------------------------|
| **Streaming Method** | VAD chunking | **AlignAtt** (cross-attention) |
| **Latency** | ~2-3s (chunk-based) | **~0.8-1.0s** (token-level) |
| **Platform** | CoreML | MLX |
| **Open Source** | Partial (Pro for full streaming) | âœ… Fully open |

### Key Advantage of AlignAtt

- WhisperKit waits for VAD chunk completion before decoding
- AlignAtt emits tokens **during decoding** by monitoring cross-attention weights
- Result: ~2-3x lower latency to first word

## Components to Port

### Python â†’ Swift Mapping (~3,109 lines total)

```
Python Component              Swift Equivalent              Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
streaming.py (282 LoC)        StreamingDecoder.swift        ğŸ†• New
â”œâ”€ StreamingConfig            struct StreamingConfig
â”œâ”€ StreamingResult            struct StreamingResult
â”œâ”€ get_most_attended_frame()  func getMostAttendedFrame()
â”œâ”€ should_emit()              func shouldEmit()
â””â”€ StreamingDecoder           class StreamingDecoder

whisper.py (975 LoC)          WhisperModel.swift            ğŸ†• New
â”œâ”€ AudioEncoder               class AudioEncoder
â”œâ”€ TextDecoder                class TextDecoder
â”œâ”€ MultiHeadAttention         class MultiHeadAttention
â””â”€ generate_streaming()       func generateStreaming()

audio.py (82 LoC)             AudioProcessor.swift          ğŸ†• New
â””â”€ log_mel_spectrogram()      func logMelSpectrogram()

decoding.py (767 LoC)         Decoding.swift                ğŸ†• New
â”œâ”€ DecodingOptions            struct DecodingOptions
â”œâ”€ DecodingTask               class DecodingTask
â””â”€ GreedyDecoder              class GreedyDecoder

tokenizer.py (398 LoC)        Use tiktoken-swift            âœ… Exists
```

## Effort Estimation

| Phase | Component | Complexity | Time |
|-------|-----------|-----------|-------|
| **1** | AudioEncoder + Mel Spectrogram | Medium | 3-4 days |
| **2** | TextDecoder + MultiHeadAttention | High | 5-7 days |
| **3** | Decoding (GreedyDecoder) | Medium | 3-4 days |
| **4** | **StreamingDecoder (AlignAtt)** | Medium | 2-3 days |
| **5** | Integration + Tests | Medium | 3-4 days |
| | **Total** | | **2-3 weeks** |

## Proposed Swift Architecture

```
MLXAudioSTT/
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ Whisper/
â”‚       â”œâ”€â”€ WhisperModel.swift        // AudioEncoder + TextDecoder
â”‚       â”œâ”€â”€ MultiHeadAttention.swift  // With cross-attention capture
â”‚       â”œâ”€â”€ Streaming/
â”‚       â”‚   â”œâ”€â”€ StreamingConfig.swift
â”‚       â”‚   â”œâ”€â”€ StreamingResult.swift
â”‚       â”‚   â””â”€â”€ StreamingDecoder.swift // AlignAtt logic
â”‚       â””â”€â”€ Decoding/
â”‚           â”œâ”€â”€ DecodingOptions.swift
â”‚           â””â”€â”€ GreedyDecoder.swift
â”œâ”€â”€ Audio/
â”‚   â”œâ”€â”€ AudioProcessor.swift          // Mel spectrogram
â”‚   â””â”€â”€ MicrophoneCapture.swift       // Real-time input
â””â”€â”€ Utils/
    â””â”€â”€ WhisperTokenizer.swift        // tiktoken wrapper
```

## Streaming Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMING PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Microphone â”€â”€â–º Mel Spectrogram â”€â”€â–º AudioEncoder â”€â”€â”           â”‚
â”‚     (AVFoundation)   (vDSP/MLX)       (MLX)        â”‚           â”‚
â”‚                                                     â–¼           â”‚
â”‚                                              KV Cache           â”‚
â”‚                                                     â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  â”‚                                                              â”‚
â”‚  â–¼                                                              â”‚
â”‚  TextDecoder â”€â”€â–º Cross-Attention â”€â”€â–º AlignAtt â”€â”€â–º Emit Token   â”‚
â”‚    (MLX)         Weights capture      Check        if stable    â”‚
â”‚                                                                 â”‚
â”‚  Target Latency: < 500ms to first token                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core AlignAtt Implementation (Swift)

```swift
// StreamingDecoder.swift - Core AlignAtt logic

import MLX
import MLXNN

public struct StreamingConfig {
    var frameThreshold: Int = 25      // Frames from audio end before emitting
    var minChunkDuration: Float = 0.5 // Minimum chunk duration in seconds
    var emitPartial: Bool = true      // Emit partial results
}

public struct StreamingResult {
    var text: String
    var tokens: [Int]
    var isFinal: Bool
    var startTime: Float
    var endTime: Float
    var progress: Float           // 0.0 to 1.0
    var audioPosition: Float      // Current position in seconds
    var audioDuration: Float      // Total duration in seconds
}

/// Extract the most attended audio frame from cross-attention weights
/// Uses alignment heads for word-level timestamp accuracy
public func getMostAttendedFrame(
    crossQK: [MLXArray],
    alignmentHeads: MLXArray
) -> Int {
    // Stack attention weights from alignment heads
    let weights = MLX.stack(
        alignmentHeads.map { pair in
            let layer = Int(pair[0].item(Int.self))
            let head = Int(pair[1].item(Int.self))
            return crossQK[layer][0, head, -1, ...]  // Last token's attention
        }
    )

    // Average across heads and find max
    let avgAttention = weights.mean(axis: 0)
    return Int(MLX.argmax(avgAttention).item(Int.self))
}

/// Determine if current token should be emitted based on attention stability
public func shouldEmit(
    mostAttendedFrame: Int,
    contentFrames: Int,
    threshold: Int
) -> Bool {
    return (contentFrames - mostAttendedFrame) >= threshold
}
```

## Benchmark Results (Python Reference)

Performance on 30s audio with `whisper-large-v3-turbo`:

| Mode | Total Time | Time to First | RTF |
|------|------------|---------------|-----|
| Batch | 1.13s | 1.13s | 0.038 |
| Streaming (0.5s chunks) | 34.9s | **1.00s** | 1.16 |
| Streaming (1.0s chunks) | 18.2s | **1.00s** | 0.61 |
| Streaming (2.0s chunks) | 9.9s | **1.13s** | 0.33 |
| Streaming (3.0s chunks) | 6.7s | **1.15s** | 0.22 |

**Key insight**: Streaming latency is independent of audio length (~1s to first result).

## Implementation Options

### Option A: Full MLX Swift Port (2-3 weeks)
- âœ… Full control over latency
- âœ… Native AVFoundation integration
- âœ… Can be added to mlx-audio-swift
- âŒ More work

### Option B: WhisperKit + AlignAtt Patch (1 week)
- âœ… Faster implementation
- âŒ CoreML dependency (less flexible)
- âŒ WhisperKit Pro required for full streaming

### Option C: Hybrid - Python backend + Swift UI (3-5 days)
- âœ… Use existing streaming implementation
- âœ… Swift UI for iOS/macOS
- âŒ Not fully native

## References

- [AlignAtt Paper](https://arxiv.org/abs/2211.00895) - SimulMT with AlignAtt
- [WhisperKit](https://github.com/argmaxinc/WhisperKit) - Swift Whisper implementation
- [MLX Swift](https://github.com/ml-explore/mlx-swift) - Apple's ML framework for Swift
- [mlx-audio](https://github.com/Blaizzy/mlx-audio) - Python reference implementation

## Next Steps

1. [ ] Create `MLXAudioSTT` Swift package structure
2. [ ] Port `AudioProcessor` (mel spectrogram)
3. [ ] Port `MultiHeadAttention` with cross-attention capture
4. [ ] Port `AudioEncoder` and `TextDecoder`
5. [ ] Implement `StreamingDecoder` with AlignAtt
6. [ ] Add `MicrophoneCapture` for real-time input
7. [ ] Integration tests and benchmarks
8. [ ] iOS/macOS demo app

---

*Document created: January 2025*
*Branch: feat/streaming-stt*
